{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake_News_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI9jhXKPCFcJ",
        "colab_type": "text"
      },
      "source": [
        "# Stance Detection for the Fake News Challenge\n",
        "\n",
        "## Identifying Textual Relationships with Deep Neural Nets\n",
        "\n",
        "### Check the problem context [here](http://www.fakenewschallenge.org/).\n",
        "\n",
        "### Download files required for the project from [here](https://drive.google.com/drive/folders/1D6YRfeXGg5k8GaMCwQWOc16pqiSFi6bC?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGwVS1UaO9LF",
        "colab_type": "text"
      },
      "source": [
        " ## <font color=red> Milestone - 1 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSNgdEMpenpE",
        "colab_type": "text"
      },
      "source": [
        "## Step1: Load the given dataset <h1> [10 marks] </h1>\n",
        "\n",
        "1. Mount the google drive\n",
        "\n",
        "2. Import Glove embeddings\n",
        "\n",
        "3. Import the test and train datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPOZRohMiSpQ",
        "colab_type": "text"
      },
      "source": [
        "### Mount the google drive to access required project files\n",
        "\n",
        "Run the below commands"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AS39z1XgFpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_7yCFdzgFsH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "5e121af3-1df9-401f-e8a8-ca5dfba28d6e"
      },
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhZdJ4zpwWzN",
        "colab_type": "text"
      },
      "source": [
        "#### Path for Project files on google drive\n",
        "\n",
        "**Note:** You need to change this path according where you have kept the files in google drive. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aol97RUogFuS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "project_path = \"/content/drive/My Drive/Datasets/Fake News Challenge-20190812T030358Z-001/Fake News Challenge/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ly0VxAnwJ2f",
        "colab_type": "text"
      },
      "source": [
        "### Loading the Glove Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmsPn6PF-cgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'glove.6B.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjLJEQ_PwcGi",
        "colab_type": "text"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "1. Using [read_csv()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) in pandas load the given train datasets files **`train_bodies.csv`** and **`train_stances.csv`**\n",
        "\n",
        "2. Using [merge](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html) command in pandas merge the two datasets based on the Body ID. \n",
        "\n",
        "Note: Save the final merged dataset in a dataframe with name **`dataset`**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gXO1WZ-gFwm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "d8bd3406-8fb6-43e8-8017-d8fee623e826"
      },
      "source": [
        "#load train_bodies.csv\n",
        "import pandas as pd\n",
        "path = '/content/drive/My Drive/Datasets/Fake News Challenge-20190812T030358Z-001/Fake News Challenge/train_bodies.csv'\n",
        "bodies = pd.read_csv(path, encoding='latin1')\n",
        "bodies = bodies.fillna(method=\"ffill\") # Deal with N/A\n",
        "bodies = bodies.sort_values('Body ID')\n",
        "bodies.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>(NEWSER) â Wonder how long a Quarter Pounder...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>Posting photos of a gun-toting child online, I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID                                        articleBody\n",
              "0        0  A small meteorite crashed into a wooded area i...\n",
              "1        4  Last week we hinted at what was to come as Ebo...\n",
              "2        5  (NEWSER) â Wonder how long a Quarter Pounder...\n",
              "3        6  Posting photos of a gun-toting child online, I...\n",
              "4        7  At least 25 suspected Boko Haram insurgents we..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrNnyN-6bW-v",
        "colab_type": "text"
      },
      "source": [
        "Loaded Article Bodies:\n",
        "\n",
        "\tBody ID articleBody\n",
        "0\t0       A small meteorite crashed into a wooded area i...\n",
        "\n",
        "1\t4\t      Last week we hinted at what was to come as Ebo...\n",
        "\n",
        "2\t5\t      (NEWSER) â Wonder how long a Quarter Pounder...\n",
        "\n",
        "3\t6\t      Posting photos of a gun-toting child online, I...\n",
        "\n",
        "4\t7\t      At least 25 suspected Boko Haram insurgents we..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kosAWskdOOT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 886
        },
        "outputId": "015fbca3-77f2-406e-de10-5322faea72a4"
      },
      "source": [
        "#load train_stances.csv\n",
        "path = '/content/drive/My Drive/Datasets/Fake News Challenge-20190812T030358Z-001/Fake News Challenge/train_stances.csv'\n",
        "stances = pd.read_csv(path, encoding='latin1')\n",
        "stances = stances.fillna(method=\"ffill\") # Deal with N/A\n",
        "stances = stances.sort_values('Body ID')\n",
        "stances.head(27)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Headline</th>\n",
              "      <th>Body ID</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27879</th>\n",
              "      <td>Soldier shot near Canadian parliament building</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21704</th>\n",
              "      <td>Caught a catfish record in Po: 127 kg and 2.67...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7110</th>\n",
              "      <td>Enormous 20-stone catfish caught with fishing ...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12573</th>\n",
              "      <td>Soldier shot at war memorial in Canada</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16307</th>\n",
              "      <td>A soldier has been shot at Canadaâs war memo...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37891</th>\n",
              "      <td>Canadian Soldier Shot At Ottawa War Memorial: ...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37896</th>\n",
              "      <td>Iraqi social-media rumors claim IS leader slain</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35767</th>\n",
              "      <td>Breaking: Soldier shot at National War Memoria...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44961</th>\n",
              "      <td>Kurds fear Isis use of chemical weapon in Kobani</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4740</th>\n",
              "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7355</th>\n",
              "      <td>Italian catches huge wels catfish; is it a rec...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4270</th>\n",
              "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2488</th>\n",
              "      <td>Tourist dubbed âSpider Manâ after spider b...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44936</th>\n",
              "      <td>BREAKING NEWS: Gunman 'shot dead' after woundi...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22015</th>\n",
              "      <td>Monster catfish which looks big enough to swal...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7857</th>\n",
              "      <td>Not coming to a store near you: The pumpkin sp...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2460</th>\n",
              "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29994</th>\n",
              "      <td>Soldier shot in Ottawa at War Memorial</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26052</th>\n",
              "      <td>Apple Watch to Be Shower-Proof, Have 100,000 A...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35971</th>\n",
              "      <td>Google to buy big chunk of Pacific Shores, ico...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24543</th>\n",
              "      <td>Luke Somers' sister says he was killed in fail...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12785</th>\n",
              "      <td>Surreal Photos of Fishermanâs Jaw-Dropping C...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13724</th>\n",
              "      <td>Fisherman lands 19 STONE catfish which could b...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34354</th>\n",
              "      <td>Comcast Is Threatening To Cut Off Customers Wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42216</th>\n",
              "      <td>There has been a shooting at the War Memorial ...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34486</th>\n",
              "      <td>Small Meteorite Strikes in Nicaragua's Capital...</td>\n",
              "      <td>0</td>\n",
              "      <td>agree</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2682</th>\n",
              "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
              "      <td>0</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Headline  Body ID     Stance\n",
              "27879     Soldier shot near Canadian parliament building        0  unrelated\n",
              "21704  Caught a catfish record in Po: 127 kg and 2.67...        0  unrelated\n",
              "7110   Enormous 20-stone catfish caught with fishing ...        0  unrelated\n",
              "12573             Soldier shot at war memorial in Canada        0  unrelated\n",
              "16307  A soldier has been shot at Canadaâs war memo...        0  unrelated\n",
              "37891  Canadian Soldier Shot At Ottawa War Memorial: ...        0  unrelated\n",
              "37896    Iraqi social-media rumors claim IS leader slain        0  unrelated\n",
              "35767  Breaking: Soldier shot at National War Memoria...        0  unrelated\n",
              "44961   Kurds fear Isis use of chemical weapon in Kobani        0  unrelated\n",
              "4740   Giant 8ft 9in catfish weighing 19 stone caught...        0  unrelated\n",
              "7355   Italian catches huge wels catfish; is it a rec...        0  unrelated\n",
              "4270    BREAKING: Soldier shot at War Memorial in Ottawa        0  unrelated\n",
              "2488   Tourist dubbed âSpider Manâ after spider b...        0  unrelated\n",
              "44936  BREAKING NEWS: Gunman 'shot dead' after woundi...        0  unrelated\n",
              "22015  Monster catfish which looks big enough to swal...        0  unrelated\n",
              "7857   Not coming to a store near you: The pumpkin sp...        0  unrelated\n",
              "2460   Soldier shot, Parliament locked down after gun...        0  unrelated\n",
              "29994             Soldier shot in Ottawa at War Memorial        0  unrelated\n",
              "26052  Apple Watch to Be Shower-Proof, Have 100,000 A...        0  unrelated\n",
              "35971  Google to buy big chunk of Pacific Shores, ico...        0  unrelated\n",
              "24543  Luke Somers' sister says he was killed in fail...        0  unrelated\n",
              "12785  Surreal Photos of Fishermanâs Jaw-Dropping C...        0  unrelated\n",
              "13724  Fisherman lands 19 STONE catfish which could b...        0  unrelated\n",
              "34354  Comcast Is Threatening To Cut Off Customers Wh...        0  unrelated\n",
              "42216  There has been a shooting at the War Memorial ...        0  unrelated\n",
              "34486  Small Meteorite Strikes in Nicaragua's Capital...        0      agree\n",
              "2682   Luke Somers 'killed in failed rescue attempt i...        0  unrelated"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVMZXXLTcJJH",
        "colab_type": "text"
      },
      "source": [
        "Loaded HeadLines:\n",
        "\n",
        "\tHeadline\tBody ID\tStance\n",
        "27879\tSoldier shot near Canadian parliament building\t0\tunrelated\n",
        "\n",
        "21704\tCaught a catfish record in Po: 127 kg and 2.67...\t0\tunrelated\n",
        "\n",
        "7110\tEnormous 20-stone catfish caught with fishing ...\t0\tunrelated\n",
        "\n",
        "12573\tSoldier shot at war memorial in Canada\t0\tunrelated\n",
        "\n",
        "16307\tA soldier has been shot at Canadaâs war memo...\t0\tunrelated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq8Heb86uvPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#merge the bodies and stances into dataset\n",
        "dataset=pd.merge(bodies, stances, on='Body ID',sort=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4ycQbBCg20S",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "<h2> Check1:</h2>\n",
        "  \n",
        "<h3> See the data: </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUtF7iOmj11k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2470d0ac-8fcf-4ad2-94f7-62a9f3993948"
      },
      "source": [
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Body ID</th>\n",
              "      <th>articleBody</th>\n",
              "      <th>Headline</th>\n",
              "      <th>Stance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot near Canadian parliament building</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Caught a catfish record in Po: 127 kg and 2.67...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Enormous 20-stone catfish caught with fishing ...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>Soldier shot at war memorial in Canada</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>A small meteorite crashed into a wooded area i...</td>\n",
              "      <td>A soldier has been shot at Canadaâs war memo...</td>\n",
              "      <td>unrelated</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Body ID  ...     Stance\n",
              "0        0  ...  unrelated\n",
              "1        0  ...  unrelated\n",
              "2        0  ...  unrelated\n",
              "3        0  ...  unrelated\n",
              "4        0  ...  unrelated\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsRxDxDSccew",
        "colab_type": "text"
      },
      "source": [
        "Merged Dataset:\n",
        "\n",
        "\tBody ID\tarticleBody\tHeadline\tStance\n",
        "0\t0\tA small meteorite crashed into a wooded area i...\tSoldier shot near Canadian parliament building\tunrelated\n",
        "\n",
        "1\t0\tA small meteorite crashed into a wooded area i...\tCaught a catfish record in Po: 127 kg and 2.67...\tunrelated\n",
        "\n",
        "2\t0\tA small meteorite crashed into a wooded area i...\tEnormous 20-stone catfish caught with fishing ...\tunrelated\n",
        "\n",
        "3\t0\tA small meteorite crashed into a wooded area i...\tSoldier shot at war memorial in Canada\tunrelated\n",
        "\n",
        "4\t0\tA small meteorite crashed into a wooded area i...\tA soldier has been shot at Canadaâs war memo...\tunrelated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjzVz2ifijmj",
        "colab_type": "text"
      },
      "source": [
        "## Step2: Data Pre-processing and setting some hyper parameters needed for model\n",
        "\n",
        "\n",
        "#### Run the code given below to set the required parameters.\n",
        "\n",
        "1. `MAX_SENTS` = Maximum no.of sentences to consider in an article.\n",
        "\n",
        "2. `MAX_SENT_LENGTH` = Maximum no.of words to consider in a sentence.\n",
        "\n",
        "3. `MAX_NB_WORDS` = Maximum no.of words in the total vocabualry.\n",
        "\n",
        "4. `MAX_SENTS_HEADING` = Maximum no.of sentences to consider in a heading of an article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDXSdpvqjuqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_NB_WORDS = 20000\n",
        "MAX_SENTS = 20\n",
        "MAX_SENTS_HEADING = 1\n",
        "MAX_SENT_LENGTH = 20\n",
        "VALIDATION_SPLIT = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwE7CPHdiDT-",
        "colab_type": "text"
      },
      "source": [
        "### Download the `Punkt` from nltk for sentence tokenization.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsiKmyJUZ-hU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "69f99f51-9834-4af8-9f52-8435519e9bd5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gqwm_GbwwnhX",
        "colab_type": "text"
      },
      "source": [
        "### Tokenizing the text and loading the pre-trained Glove word embeddings for each token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLSn9S-5oG4Z",
        "colab_type": "text"
      },
      "source": [
        "#### Import the Tokenizer from keras preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-VUgh2yoMlR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "721314ea-5fc7-41af-a74e-6db85518eb17"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eml0Lge4oOuh",
        "colab_type": "text"
      },
      "source": [
        "#### Initialize the Tokenizer class with maximum vocabulary count as `MAX_NB_WORDS`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm85qirPofc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initializing tokenizer t with MAX_NB_WORDS as the maximum number of words\n",
        "t = Tokenizer(num_words=MAX_NB_WORDS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBe1KuXDosJ7",
        "colab_type": "text"
      },
      "source": [
        "#### Now, using fit_on_texts() from Tokenizer class,encode the data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5rk-UyBlmyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting all the words in article body and headline into a list - text\n",
        "headlines=list(dataset['Headline'])\n",
        "body=list(dataset['articleBody'])\n",
        "text = headlines + body"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9EABDfz1CsM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10df7b99-a534-4f45-e3a2-fd5d8b3ed76e"
      },
      "source": [
        "print(bodies['articleBody'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       A small meteorite crashed into a wooded area i...\n",
            "1       Last week we hinted at what was to come as Ebo...\n",
            "2       (NEWSER) â Wonder how long a Quarter Pounder...\n",
            "3       Posting photos of a gun-toting child online, I...\n",
            "4       At least 25 suspected Boko Haram insurgents we...\n",
            "5       There is so much fake stuff on the Internet in...\n",
            "6       (CNN) -- A meteorite crashed down in Managua, ...\n",
            "7       Move over, Netflix and Hulu.\\nWord has it that...\n",
            "8       Weâve all seen the traditional depictions of...\n",
            "9       A SOLDIER has been shot at Canadaâs National...\n",
            "10      mboxCreate('FoxNews-Politics-Autoplay-Videos-I...\n",
            "11      Don't fucking cheat on Cassy, aka @NessLovnTre...\n",
            "12      Kai the shar pei-crossbreed was discovered tie...\n",
            "13      An article saying NASA confirmed six days of â...\n",
            "14      Italian fisherman Dino Ferrari landed what cou...\n",
            "15      HBO's subscription streaming service will be c...\n",
            "16      In a sprawling Facebook post and subsequent in...\n",
            "17      Macaulay Culkin is not dead. The actor is just...\n",
            "18      DUBAI - A prominent Saudi Arabian cleric has w...\n",
            "19      Eran Cicurel, an editor at Voice of Israel, ha...\n",
            "20      Phoenix, AZ â A Phoenix boy is behind bars t...\n",
            "21      Abdel-Majed Abdel Bary is a former West London...\n",
            "22      DNA tests confirm Lebanon is holding the young...\n",
            "23      FERGUSON, St. Louis (CNN) â Could a newly re...\n",
            "24      Abdel-Majed Abdel Bary, who went by the rap na...\n",
            "25      New Delhi: AK Verma, an executive engineer at ...\n",
            "26      The London Metropolitan Police has denied repo...\n",
            "27      Young North Korean dictator Kim Jong Unâs he...\n",
            "28      Although Apple has given us our first peek at ...\n",
            "29      The small town of Purdon, Texas has been quara...\n",
            "                              ...                        \n",
            "1653    Offers have been pouring in to re-home a dog f...\n",
            "1654    Although Apple has given us our first peek at ...\n",
            "1655    Sorry to disappoint, fans of Led Zeppelin, but...\n",
            "1656    While other global leaders rant their exaspera...\n",
            "1657    SAN SEVERINO - Go to the hospital accusing a t...\n",
            "1658    Islamic State militants appear to have killed ...\n",
            "1659    Warning graphic image: Oliver Ilic, 22, had to...\n",
            "1660    If 9to5Macâs latest findings are true, than ...\n",
            "1661    In case you missed it, Vogue Magazine, one of ...\n",
            "1662    In response to Ebola Scare in Kansas City :\\nV...\n",
            "1663    News that the âHome Aloneâ star has allege...\n",
            "1664    Seven girls aged between 13 and 14 have fallen...\n",
            "1665    Fidel Castro was pronounced dead on Twitter so...\n",
            "1666    28 girls, aged 13 and 14, from a small town we...\n",
            "1667    BEIRUT â Islamic State group fighters seized...\n",
            "1668    The weather man was reporting from Sugar Mount...\n",
            "1669    Soâ¦ Rebecca Schoenkopf over at Wonkette is p...\n",
            "1670    Et si Dieu Ã©tait une femme? C'est ce qu'affir...\n",
            "1671    On Friday, a rumor cropped up that one of the ...\n",
            "1672    Anna Wintour, editor-in-chief on Vogue magazin...\n",
            "1673    The hoax story has been circulating far and wi...\n",
            "1674    THE hunt is on to find the owner of a dog who ...\n",
            "1675    President Obama said Wednesday that the United...\n",
            "1676    Oh Internet, when will you ever stop killing p...\n",
            "1677    Update: Since the publication of this article,...\n",
            "1678    Intelligence agencies hunting for identity of ...\n",
            "1679    While Daleks \"know no fear\" and \"must not fear...\n",
            "1680    More than 200 schoolgirls were kidnapped in Ap...\n",
            "1681    A Guantanamo Bay prisoner released last year a...\n",
            "1682    ANN ARBOR, Mich. â A pizza delivery man in M...\n",
            "Name: articleBody, Length: 1683, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70iEytanvOWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b5f93759-6cf8-41c8-9121-ac8bb10fd68e"
      },
      "source": [
        "#displaying the first 5 articles in text list\n",
        "print(text[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Soldier shot near Canadian parliament building', 'Caught a catfish record in Po: 127 kg and 2.67 meters', 'Enormous 20-stone catfish caught with fishing rod in Italy after 40-minute boat battle', 'Soldier shot at war memorial in Canada', 'A soldier has been shot at Canadaâ\\x80\\x99s war memorial just steps away from the nationâ\\x80\\x99s parliament']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN6R0Yv2cwD8",
        "colab_type": "text"
      },
      "source": [
        "First 5 articles in text list:\n",
        "\n",
        "[['Soldier shot near Canadian parliament building', \n",
        "'Caught a catfish record in Po: 127 kg and 2.67 meters', 'Enormous 20-stone catfish caught with fishing rod in Italy after 40-minute boat battle',\n",
        "'Soldier shot at war memorial in Canada', \n",
        "'A soldier has been shot at Canadaâ\\x80\\x99s war memorial just steps away from the nationâ\\x80\\x99s parliament']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OnEeyaOvTNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#fitting the tokenizer on the words from headlines and articleBody\n",
        "t.fit_on_texts(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omptHX-JpBsN",
        "colab_type": "text"
      },
      "source": [
        "#### fit_on_texts() gives the following attributes:\n",
        "\n",
        "* **word_counts:** dictionary mapping words (str) to the number of times they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_docs:** dictionary mapping words (str) to the number of documents/texts they appeared on during fit. Only set after fit_on_texts was called.\n",
        "\n",
        "* **word_index:** dictionary mapping words (str) to their rank/index (int). Only set after fit_on_texts was called.\n",
        "\n",
        "* **document_count:** int. Number of documents (texts/sequences) the tokenizer was trained on. Only set after fit_on_texts or fit_on_sequences was called.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHnsT2sTtFAA",
        "colab_type": "text"
      },
      "source": [
        "### Now, tokenize the sentences using nltk sent_tokenize() and encode the senteces with the ids we got form the above `t.word_index`\n",
        "\n",
        "Initialise 2 lists with names `texts` and `articles`.\n",
        "\n",
        "```\n",
        "texts = [] to store text of article as it is.\n",
        "\n",
        "articles = [] split the above text into a list of sentences.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctEu-d4c4EZs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cd8810d-5e9e-4184-cef9-94eb798a90fa"
      },
      "source": [
        "#initialise a list texts with all the articles from the dataset\n",
        "texts= list(dataset['articleBody'])\n",
        "#displaying the first article\n",
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edLz9GtpdaAs",
        "colab_type": "text"
      },
      "source": [
        "The First Article:\n",
        "\n",
        "'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k06TKIXqvlX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits the articles into sentences\n",
        "articles=[]\n",
        "#for each article among all articles, tokenize the article and add it to the list of tokenized articles\n",
        "for text in texts:\n",
        "    article = nltk.tokenize.sent_tokenize(text)\n",
        "    articles.append(article)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koTVJjoO6P78",
        "colab_type": "text"
      },
      "source": [
        "## Check 2:\n",
        "\n",
        "first element of texts and articles should be as given below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mWBW99p5UW9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "260c219a-51dd-4de9-d475-b0bd255829be"
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BEPRsHX7eBjv"
      },
      "source": [
        "The First Article:\n",
        "\n",
        "'A small meteorite crashed into a wooded area in Nicaragua\\'s capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city\\'s airport, the Associated Press reports. \\n\\nGovernment spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \\nMurillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\\n\\nThe crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\\n\\nHumberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\\n\\n\"We have to study it more because it could be ice or rock,\" he said.\\n\\nWilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\\n\\nLocal residents reported hearing a loud boom Saturday night, but said they didn\\'t see anything strange in the sky.\\n\\n\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\\n\\nThe site of the crater is near Managua\\'s international airport and an air force base. Only journalists from state media were allowed to visit it.'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtIjO3ht5EKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9770ffb1-fcb8-4409-fd14-7c4e3f71bcc8"
      },
      "source": [
        "articles[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday.\",\n",
              " \"Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports.\",\n",
              " 'Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\"',\n",
              " 'House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports.',\n",
              " 'Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.',\n",
              " 'The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee.',\n",
              " 'He said it is still not clear if the meteorite disintegrated or was buried.',\n",
              " 'Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.',\n",
              " '\"We have to study it more because it could be ice or rock,\" he said.',\n",
              " 'Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light.',\n",
              " 'We have to ask if anyone has a photo or something.\"',\n",
              " \"Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\",\n",
              " '\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast.',\n",
              " 'We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.',\n",
              " \"The site of the crater is near Managua's international airport and an air force base.\",\n",
              " 'Only journalists from state media were allowed to visit it.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFg67fGyeSwG",
        "colab_type": "text"
      },
      "source": [
        "The First Article after Sentence Tokenization:\n",
        "\n",
        "[\"A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday.\",\n",
        "\n",
        " \"Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports.\",\n",
        " \n",
        " 'Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\"',\n",
        " \n",
        " 'House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports.',\n",
        " \n",
        " 'Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.',\n",
        " \n",
        " 'The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee.',\n",
        " \n",
        " 'He said it is still not clear if the meteorite disintegrated or was buried.',\n",
        " \n",
        " 'Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.',\n",
        " \n",
        " '\"We have to study it more because it could be ice or rock,\" he said.',\n",
        " \n",
        " 'Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light.',\n",
        " \n",
        " 'We have to ask if anyone has a photo or something.\"',\n",
        " \"Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\",\n",
        " \n",
        " '\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast.',\n",
        " \n",
        " 'We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.',\n",
        " \n",
        " \"The site of the crater is near Managua's international airport and an air force base.\",\n",
        " \n",
        " 'Only journalists from state media were allowed to visit it.']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eghvJHZSQOYY",
        "colab_type": "text"
      },
      "source": [
        " ## <font color=red> Milestone - 2 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpuRIA7cCfcY",
        "colab_type": "text"
      },
      "source": [
        "#### Now iterate through each article and each sentence to encode the words into ids using t.word_index\n",
        "\n",
        "Use use `text_to_word_sequence` to get words from sentence  \n",
        "\n",
        "1. Import text_to_word_sequence\n",
        "\n",
        "2. Initialize a variable of shape (no.of articles, MAX_SENTS, MAX_SENT_LENGTH)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVyClBULCqWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import text_to_word_sequence\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M98KE0h9vw3u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e1f1ad9-f654-4ddc-db9f-e5ff2b30a375"
      },
      "source": [
        "#find Number of articles\n",
        "no_of_articles=len(articles)\n",
        "print(no_of_articles)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbrzdy_e0hn",
        "colab_type": "text"
      },
      "source": [
        "Number of articles = 49972"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maJ8NBK4v-of",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ec99734e-872a-4ee4-9380-9724f9f5ab50"
      },
      "source": [
        "#initialize data\n",
        "import numpy as np\n",
        "data=np.zeros(shape=(no_of_articles,MAX_SENTS,MAX_SENT_LENGTH),dtype='int32')\n",
        "print(data[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D13zpMwfwBua",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this code block will find the embeddings for the body of each article and append it to the data[article]\n",
        "#for each article in range 0,no_of_articles.\n",
        "import keras\n",
        "article = 0\n",
        "for article in range(0,no_of_articles):\n",
        "  \n",
        "    #print(article)\n",
        "    #list the words in each article\n",
        "    article_value=articles[article]\n",
        "    for sentence in article_value:\n",
        "      text_headings= text_to_word_sequence(sentence)\n",
        "      #print(text_headings)\n",
        "    #generate embeddings for words used in each article\n",
        "    text_headings_embedded = (t.texts_to_sequences(articles[article]))\n",
        "    \n",
        "    #make the embeddigs to size 20x20 for each article\n",
        "    encoded_data = keras.preprocessing.sequence.pad_sequences(text_headings_embedded, maxlen=MAX_SENT_LENGTH, dtype='int32', padding='post', truncating='post', value=0.0)\n",
        "    #print(encoded_data)\n",
        "    for encoding in range(0,min(20,len(encoded_data))):\n",
        "      data[article][encoding]=encoded_data[encoding]\n",
        "      \n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFdmiDYcE144",
        "colab_type": "text"
      },
      "source": [
        "### Check 3:\n",
        "\n",
        "Accessing first element in data should give something like given below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsFWW5C2Djog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e3a0d8b-5383-4c78-8b95-a3e0171afcb1"
      },
      "source": [
        "#data.astype('int32')\n",
        "data[0, :, :]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[    3,   481,   427,  7211,    81,     3,  3734,   331,     5,\n",
              "         3892,   350,     4,  1431,  2960,     1,    89,    12,   466,\n",
              "            0,     0],\n",
              "       [  758,    95,  1047,     3,  2679,  1752,     7,   189,     3,\n",
              "         1217,  1075,  2030,   700,   159,     1,  3033,   448,     1,\n",
              "          555,   235],\n",
              "       [   89,  1068,  4117,  2349,    12,     3,  1092,  3307,    19,\n",
              "            1,    89,     2,  1793,     1,   521,  2009,    15,     9,\n",
              "            3,  3111],\n",
              "       [  181,  3641,   972,   200,  2558,    44,  6776,  1722,  1252,\n",
              "            5, 13324, 17943,     1,   778,    31,   740,  3991,    67,\n",
              "           85,     0],\n",
              "       [ 2349,    12,  1557,    38,  1094,   351,   775,     2,   367,\n",
              "          260,  1770,     5,  4455,    70,   494,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    1,   700,   189,    19,     1,   427,    32,     3,  7423,\n",
              "            4,  2159,  1252,     6,     3,  5271,     4,  1217,  1252,\n",
              "           12,  3365],\n",
              "       [   13,    12,    15,     8,   149,    25,   543,    64,     1,\n",
              "          427,  3727,    41,     9,  1850,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [ 3365,  5734,     4,     1,  5876,   614,    21,     1,   311,\n",
              "         3439,   795,     4,  1557,    12,     1,   427,    69,    23,\n",
              "          787,     2],\n",
              "       [   37,    17,     2,  1793,    15,    52,   120,    15,    69,\n",
              "           23,  4923,    41,  1963,    13,    12,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [ 4737,  3339,    24,  3971,     2,     1,  1316,     4,  3073,\n",
              "         1655,    12,    15,     9,   195,  1421,     7,    58,    40,\n",
              "           95,     3],\n",
              "       [   37,    17,     2,  1094,    64,   510,    20,     3,   250,\n",
              "           41,   264,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [  260,   758,    95,  1047,     3,  1808,  1752,   531,   276,\n",
              "           29,    12,    33,   703,   163,   893,  1421,     5,     1,\n",
              "         2081,     0],\n",
              "       [   35,     9,  2058,    10,   116,  5828,     6,    35,   576,\n",
              "          656,   104,    59,     4,     3,  2411,    35,   241,     3,\n",
              "          512,  1911],\n",
              "       [   37,   341,    15,     9,     3,  2082,   120,    37,   881,\n",
              "           24,  4456,  2585,  4317,  4924,    55,     1,   555,   235,\n",
              "            0,     0],\n",
              "       [    1,   255,     4,     1,   700,     8,   159,  3961,   351,\n",
              "          448,     6,    24,   155,   465,  1930,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [  126,   921,    22,    47,   100,    36,  1834,     2,  1213,\n",
              "           15,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0],\n",
              "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "            0,     0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Get44Nx5fI2x",
        "colab_type": "text"
      },
      "source": [
        "First Article after encoding:\n",
        "\n",
        "array([[    3,   481,   427,  7211,    81,     3,  3734,   331,     5,\n",
        "         3892,   350,     4,  1431,  2960,     1,    89,    12,   466,\n",
        "            0,     0],\n",
        "       [  758,    95,  1047,     3,  2679,  1752,     7,   189,     3,\n",
        "         1217,  1075,  2030,   700,   159,     1,  3033,   448,     1,\n",
        "          555,   235],\n",
        "       [   89,  1068,  4117,  2349,    12,     3,  1092,  3307,    19,\n",
        "            1,    89,     2,  1793,     1,   521,  2009,    15,     9,\n",
        "            3,  3111],\n",
        "       [  181,  3641,   972,   200,  2558,    44,  6776,  1722,  1252,\n",
        "            5, 13324, 17943,     1,   778,    31,   740,  3991,    67,\n",
        "           85,     0],\n",
        "       [ 2349,    12,  1557,    38,  1094,   351,   775,     2,   367,\n",
        "          260,  1770,     5,  4455,    70,   494,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [    1,   700,   189,    19,     1,   427,    32,     3,  7423,\n",
        "            4,  2159,  1252,     6,     3,  5271,     4,  1217,  1252,\n",
        "           12,  3365],\n",
        "       [   13,    12,    15,     8,   149,    25,   543,    64,     1,\n",
        "          427,  3727,    41,     9,  1850,     0,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [ 3365,  5734,     4,     1,  5876,   614,    21,     1,   311,\n",
        "         3439,   795,     4,  1557,    12,     1,   427,    69,    23,\n",
        "          787,     2],\n",
        "       [   37,    17,     2,  1793,    15,    52,   120,    15,    69,\n",
        "           23,  4923,    41,  1963,    13,    12,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [ 4737,  3339,    24,  3971,     2,     1,  1316,     4,  3073,\n",
        "         1655,    12,    15,     9,   195,  1421,     7,    58,    40,\n",
        "           95,     3],\n",
        "       [   37,    17,     2,  1094,    64,   510,    20,     3,   250,\n",
        "           41,   264,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [  260,   758,    95,  1047,     3,  1808,  1752,   531,   276,\n",
        "           29,    12,    33,   703,   163,   893,  1421,     5,     1,\n",
        "         2081,     0],\n",
        "       [   35,     9,  2058,    10,   116,  5828,     6,    35,   576,\n",
        "          656,   104,    59,     4,     3,  2411,    35,   241,     3,\n",
        "          512,  1911],\n",
        "       [   37,   341,    15,     9,     3,  2082,   120,    37,   881,\n",
        "           24,  4456,  2585,  4317,  4924,    55,     1,   555,   235,\n",
        "            0,     0],\n",
        "       [    1,   255,     4,     1,   700,     8,   159,  3961,   351,\n",
        "          448,     6,    24,   155,   465,  1930,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [  126,   921,    22,    47,   100,    36,  1834,     2,  1213,\n",
        "           15,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0],\n",
        "       [    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
        "            0,     0]], dtype=int32)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMvfFJndVyit",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fda3b64f-ab01-4835-9eaf-faa15931859a"
      },
      "source": [
        "#initialise a list texts with all the headings of articles from the dataset\n",
        "texts= list(dataset['Headline'])\n",
        "#displaying the first article Headline\n",
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Soldier shot near Canadian parliament building'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHqAXcT5fhl8",
        "colab_type": "text"
      },
      "source": [
        "First Article headline:\n",
        "'Soldier shot near Canadian parliament building'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozQEsZJfWSEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splits the headings into sentences\n",
        "headlines=[]\n",
        "#for each headline among all headings, tokenize the heading into sentences and add it to the list of tokenized headings\n",
        "for text in texts:\n",
        "    headline = nltk.tokenize.sent_tokenize(text)\n",
        "    headlines.append(headline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5RtSy64Z4YG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9d648720-e2e7-4e63-f4c1-206dab3e7076"
      },
      "source": [
        "#find Number of headlines\n",
        "no_of_headlines=len(headlines)\n",
        "print(no_of_headlines)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRk8hqmTgB_n",
        "colab_type": "text"
      },
      "source": [
        "No of headlines = 49972"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBftSvV6aPhv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a0a6c17d-b1a8-44b3-86d7-05195e395a45"
      },
      "source": [
        "#initialize headline_data\n",
        "import numpy as np\n",
        "data_heading=np.zeros(shape=(no_of_headlines,MAX_SENTS,MAX_SENT_LENGTH),dtype='int32')\n",
        "print(data_heading[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CliiIhLemJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this code block will find the embeddings for the headline of each article and append it to the headline_data[article]\n",
        "#for each headline in range 0,no_of_headlines.\n",
        "import keras\n",
        "headline = 0\n",
        "for headline in range(0,no_of_articles):\n",
        "  \n",
        "    #print(headline)\n",
        "    #list the words in each headline\n",
        "    headline_value=headlines[headline]\n",
        "    for sentence in headline_value:\n",
        "      article_headings= text_to_word_sequence(sentence)\n",
        "      #print(text_headings)\n",
        "    #generate embeddings for words used in each article\n",
        "    article_headings_embedded = (t.texts_to_sequences(headlines[headline]))\n",
        "    \n",
        "    #make the embeddigs to size 20x20 for each article\n",
        "    encoded_data = keras.preprocessing.sequence.pad_sequences(article_headings_embedded, maxlen=MAX_SENT_LENGTH, dtype='int32', padding='post', truncating='post', value=0.0)\n",
        "    #print(encoded_data)\n",
        "    for encoding in range(0,min(20,len(encoded_data))):\n",
        "      data_heading[headline][encoding]=encoded_data[encoding]\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPf5rqacb-fa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "669907aa-b469-4bb2-89c4-d087eb4478e6"
      },
      "source": [
        "print(data_heading[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[717 206 159 356 343 387   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRBtofJAgP9v",
        "colab_type": "text"
      },
      "source": [
        "First Article Heading after encoding:\n",
        "[[717 206 159 356 343 387   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]\n",
        " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
        "    0   0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaH0Ey1qe_Co",
        "colab_type": "text"
      },
      "source": [
        "### Now the features are ready, lets make the labels ready for the model to process.\n",
        "\n",
        "### Convert labels into one-hot vectors\n",
        "\n",
        "You can use [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) in pandas to create one-hot vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zq-VcgM8fat1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create one-hot encoding of target classes\n",
        "labels=pd.get_dummies(dataset['Stance'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14gZ-1pcepnt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b20885d5-07eb-45ec-e485-62fcf3a11b6d"
      },
      "source": [
        "type(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKppfkSgc1QF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "08acd1a4-2f06-4c58-f481-49dbaa641d69"
      },
      "source": [
        "#displaying the first 30 target clases\n",
        "print(labels[0:30])\n",
        "#convert labels(of type padas DataFrame) to a numpy array\n",
        "labels=labels.values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    agree  disagree  discuss  unrelated\n",
            "0       0         0        0          1\n",
            "1       0         0        0          1\n",
            "2       0         0        0          1\n",
            "3       0         0        0          1\n",
            "4       0         0        0          1\n",
            "5       0         0        0          1\n",
            "6       0         0        0          1\n",
            "7       0         0        0          1\n",
            "8       0         0        0          1\n",
            "9       0         0        0          1\n",
            "10      0         0        0          1\n",
            "11      0         0        0          1\n",
            "12      0         0        0          1\n",
            "13      0         0        0          1\n",
            "14      0         0        0          1\n",
            "15      0         0        0          1\n",
            "16      0         0        0          1\n",
            "17      0         0        0          1\n",
            "18      0         0        0          1\n",
            "19      0         0        0          1\n",
            "20      0         0        0          1\n",
            "21      0         0        0          1\n",
            "22      0         0        0          1\n",
            "23      0         0        0          1\n",
            "24      0         0        0          1\n",
            "25      1         0        0          0\n",
            "26      0         0        0          1\n",
            "27      0         0        0          1\n",
            "28      0         0        0          1\n",
            "29      0         0        0          1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxWJ0zJSgrHi",
        "colab_type": "text"
      },
      "source": [
        "Class labels read from dataframe:\n",
        "\n",
        " agree  disagree  discuss  unrelated\n",
        " \n",
        "0       0         0        0          1\n",
        "\n",
        "1       0         0        0          1\n",
        "\n",
        "2       0         0        0          1\n",
        "\n",
        "3       0         0        0          1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40mA8FI2fcxZ",
        "colab_type": "text"
      },
      "source": [
        "### Check 4:\n",
        "\n",
        "The shape of data and labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpEWEnjFfnFR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "f8b981c4-5161-4a70-a6bc-168a1399ca9b"
      },
      "source": [
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (49972, 20, 20)\n",
            "Shape of label tensor: (49972, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eX7s_VShJIL",
        "colab_type": "text"
      },
      "source": [
        "Shape of data tensor: (49972, 20, 20)\n",
        "Shape of label tensor: (49972, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDOxHdR3frDu",
        "colab_type": "text"
      },
      "source": [
        "### Shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ra-yYTvfzRt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8480e929-e5cc-4180-a43b-a7ef887a71d7"
      },
      "source": [
        "## get numbers upto no.of articles\n",
        "indices = np.arange(data.shape[0])\n",
        "## shuffle the numbers\n",
        "np.random.shuffle(indices)\n",
        "print(indices)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[28867 37484  1708 ...  8377 32283 37493]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mD3LnxUhOPS",
        "colab_type": "text"
      },
      "source": [
        "Indices of the articles after shuffling:\n",
        "[23979 43074  6633 ... 20021 16735 18750]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKnSqwIFf3Iy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## shuffle the data\n",
        "data = data[indices]\n",
        "data_heading = data_heading[indices]\n",
        "## shuffle the labels according to data\n",
        "labels = labels[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDbVz44BfKIM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf2a39fc-bcac-445f-d3a4-a1d76425a2de"
      },
      "source": [
        "#displaying the first value of data ,data_heading and labels\n",
        "print(\"data 1:\")\n",
        "print(data[0])\n",
        "print(\"headline 1:\")\n",
        "print(data_heading[0])\n",
        "print(\"label 1:\")\n",
        "print(labels[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data 1:\n",
            "[[ 5255     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [ 1132  3435     5   808     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [   78     2  2592    30   296    21     1 19849  2079  7239 18077    19\n",
            "    996   982     1  2244    56   808   954    21]\n",
            " [  160   447    24  2473  2395     2    60     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    1   179     7   296    61   163     1  3239  2442     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [  160  6170     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [   78     2     3   107    10  2475   982    30   403   117  3407    11\n",
            "   1621     1  6681   644  1134  2381    96  3987]\n",
            " [  182  2660     1  5363   756  5516   130   144   759     1    26   622\n",
            "      8    14     1  1334  1189   196   226     2]\n",
            " [  160  4005   193    61     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [ 2175  1065  8009     4   394   312     3   215   110     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [  144  2346    15    86    20 10970     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n",
            "headline 1:\n",
            "[[ 3429  7836  1428   756    21    40   154  1648   650    22  1071 15619\n",
            "      2  5363  3239     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]\n",
            " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
            "      0     0     0     0     0     0     0     0]]\n",
            "label 1:\n",
            "[0 0 1 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcOFVfPBf9kA",
        "colab_type": "text"
      },
      "source": [
        "### Split into train and validation sets. Split the train set 80:20 ratio to get the train and validation sets.\n",
        "\n",
        "\n",
        "Use the variable names as given below:\n",
        "\n",
        "x_train, x_val - for body of articles.\n",
        "\n",
        "x_heading_train, x_heading_val - for heading of articles.\n",
        "\n",
        "y_train - for training labels.\n",
        "\n",
        "y_val - for validation labels.\n",
        "\n",
        "<h1> [10 marks] </h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfJp8YuGhSuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "196218b9-483d-4c2d-94cd-ce57d1e46f4f"
      },
      "source": [
        "import math\n",
        "index1 = 0.8*len(data)\n",
        "index1=math.ceil(index1)\n",
        "index1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39978"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWTWNWEIhl4K",
        "colab_type": "text"
      },
      "source": [
        "Training set: index 0-39978 ,\n",
        "Validation set: index 39979-40000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2neh9Wcof8iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#splitting into training and validation sets in the ratio 80:20\n",
        "x_train,x_val = data[0:index1],data[index1+1:]\n",
        "x_heading_train,x_heading_val = data_heading[0:index1],data_heading[index1+1:]\n",
        "y_train,y_val = labels[0:index1],labels[index1+1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTyvoHrsgMDw",
        "colab_type": "text"
      },
      "source": [
        "### Check 5:\n",
        "\n",
        "The shape of x_train, x_val, y_train and y_val should match the below numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLEbiw2Yghe2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0c8dd0f5-55a0-44f3-8fa6-393933fe24d5"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39978, 20, 20)\n",
            "(39978, 4)\n",
            "(9993, 20, 20)\n",
            "(9993, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-IXBi0Xh4z8",
        "colab_type": "text"
      },
      "source": [
        "Shape of training data and class labels:- (39978, 20, 20) and (39978, 4)\n",
        "\n",
        "Shape of data and class labels used for validation:- (9993, 20, 20) and (9993, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNnoBtArhJ1E",
        "colab_type": "text"
      },
      "source": [
        "### Create embedding matrix with the glove embeddings\n",
        "\n",
        "\n",
        "Run the below code to create embedding_matrix which has all the words and their glove embedding if present in glove word list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKqxS5fKjbn1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07173413-c7b0-4b05-ef59-93e73b43b24a"
      },
      "source": [
        "vocab_size=len((t.word_index.items()))\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOVdqz6BigPv",
        "colab_type": "text"
      },
      "source": [
        "Vocabulary size = 27873"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKqn2IL2ZF8v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "74580622-890b-4001-f2d1-79183fe180db"
      },
      "source": [
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "f = open('./glove.6B.100d.txt')\n",
        "for line in f:\n",
        "\tvalues = line.split()\n",
        "\tword = values[0]\n",
        "\tcoefs = np.asarray(values[1:], dtype='float32')\n",
        "\tembeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Loaded %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "\n",
        "\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0KYP6pfQW9E",
        "colab_type": "text"
      },
      "source": [
        " ## <font color=red> Milestone - 3 </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRi4o3ZspDFU",
        "colab_type": "text"
      },
      "source": [
        "## Try different sequential models and report accuracy scores for each model.\n",
        "\n",
        "<h1>[50 marks]  </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSZDnPWkw2ZZ",
        "colab_type": "text"
      },
      "source": [
        "### Import layers from Keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AgwQsfMrzAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing layers and models from keras\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Embedding , Dense, Dropout, Bidirectional, Input , TimeDistributed, BatchNormalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NL6gNx4hsCUu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Flatten,Activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpkVhIbx3gr1",
        "colab_type": "text"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tblNuGSnzA1",
        "colab_type": "text"
      },
      "source": [
        "**Model 1:Without using glove embeddings**\n",
        "1. Training and testing is done using article bodies(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLtzT2IMyZWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ac367abc-66af-4a54-f554-c9caa24df660"
      },
      "source": [
        "print(x_train.shape,x_val.shape)\n",
        "type(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39978, 20, 20) (9993, 20, 20)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF-_4qZUyVyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_model1 =x_train.reshape(39978,MAX_SENTS *MAX_SENT_LENGTH)\n",
        "x_val_model1=x_val.reshape(9993,MAX_SENTS *MAX_SENT_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HRVHKB_ykxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "017572c0-38f7-459b-81d1-3327ecef0c26"
      },
      "source": [
        "print(x_train_model1.shape,x_val_model1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39978, 400) (9993, 400)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_8QXh-rmPFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Article_len = 400\n",
        "text_input_model1 = sentence_input = Input(shape=(Article_len,), dtype='int32')\n",
        "embedded_sequence_m1 = Embedding(output_dim=100, input_dim=vocab_size, input_length=(400,))(text_input_model1)\n",
        "l_lstm_m1 = Bidirectional(LSTM(100,return_sequences=True))(embedded_sequence_m1)\n",
        "l_dense_m11 = TimeDistributed(Dense(100))(l_lstm_m1)\n",
        "l_flatten_m1 = Flatten()(l_dense_m11)\n",
        "l_dense_m1 = Dense(4,activation='softmax')(l_flatten_m1)\n",
        "\n",
        "model = model = Model(inputs=text_input_model1, outputs=l_dense_m1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Xrd-JQ3id7",
        "colab_type": "text"
      },
      "source": [
        "### Compile and fit the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlduHU2CovxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glSRMU8J3wz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "96835d05-da6d-49ce-9052-42bef348dd82"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 400, 100)          2787300   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 400, 200)          160800    \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 400, 100)          20100     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 40000)             0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 4)                 160004    \n",
            "=================================================================\n",
            "Total params: 3,128,204\n",
            "Trainable params: 3,128,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM3yCmjQoCM3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "739dbef9-ca53-42e8-938f-fb859ff6fccd"
      },
      "source": [
        "model.fit(x=x_train_model1, y=y_train, epochs=2, verbose=1, validation_data=(x_val_model1,y_val), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 39978 samples, validate on 9993 samples\n",
            "Epoch 1/2\n",
            "39978/39978 [==============================] - 2018s 50ms/step - loss: 0.2822 - acc: 0.8876 - val_loss: 0.2579 - val_acc: 0.8993\n",
            "Epoch 2/2\n",
            "39978/39978 [==============================] - 2031s 51ms/step - loss: 0.2431 - acc: 0.9021 - val_loss: 0.2613 - val_acc: 0.8984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7be17afbe0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odbVZGi3s9Pk",
        "colab_type": "text"
      },
      "source": [
        "model.fit results:\n",
        "\n",
        "Train on 39978 samples, validate on 9993 samples\n",
        "\n",
        "Epoch 1/2\n",
        "39978/39978 [==============================] - 2018s 50ms/step - loss: 0.2822 - acc: 0.8876 - val_loss: 0.2579 - val_acc: 0.8993\n",
        "\n",
        "Epoch 2/2\n",
        "\n",
        "39978/39978 [==============================] - 2031s 51ms/step - loss: 0.2431 - acc: 0.9021 - val_loss: 0.2613 - val_acc: 0.8984\n",
        "\n",
        "<keras.callbacks.History at 0x7f7be17afbe0>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrgklNpDlp3V",
        "colab_type": "text"
      },
      "source": [
        "**MODEL 1:**\n",
        "    \n",
        "    This model uses only the  article bodies for prediction.The input is passed through an\n",
        "    LSTM block. It has a batch size of 10 and produces 88.44% validation accuracy after 2 epochs.\n",
        "    \n",
        "    MODEL SUMMARY:\n",
        "    ============\n",
        "    \n",
        "    \n",
        "    Model: \"model_3\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "\n",
        "input_3 (InputLayer)         (None, 400)               0         \n",
        "_________________________________________________________________\n",
        "embedding_3 (Embedding)      (None, 400, 100)          2787300   \n",
        "_________________________________________________________________\n",
        "bidirectional_3 (Bidirection (None, 400, 200)          160800    \n",
        "_________________________________________________________________\n",
        "time_distributed_3 (TimeDist (None, 400, 100)          20100     \n",
        "_________________________________________________________________\n",
        "flatten_3 (Flatten)          (None, 40000)             0         \n",
        "_________________________________________________________________\n",
        "dense_6 (Dense)              (None, 4)                 160004    \n",
        "\n",
        "Total params: 3,128,204\n",
        "Trainable params: 3,128,204\n",
        "Non-trainable params: 0\n",
        "\n",
        "\n",
        " **MODEL RESULTS:**\n",
        "  \n",
        "  \n",
        "  Train on 39978 samples, validate on 9993 samples\n",
        "Epoch 1/2\n",
        "39978/39978 [==============================] - 2018s 50ms/step - loss: 0.2822 - acc: 0.8876 - val_loss: 0.2579 - val_acc: 0.8993\n",
        "Epoch 2/2\n",
        "39978/39978 [==============================] - 2031s 51ms/step - loss: 0.2431 - acc: 0.9021 - val_loss: 0.2613 - val_acc: 0.8984\n",
        "<keras.callbacks.History at 0x7f7be17afbe0>\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S9B2ipFuMk9",
        "colab_type": "text"
      },
      "source": [
        "# Model 2:\n",
        "\n",
        "Using both headline and body for prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TUIpzzQAgT8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reshaping headline data to pass to the model\n",
        "x_train_headline =x_heading_train.reshape(39978,MAX_SENTS *MAX_SENT_LENGTH)\n",
        "x_val_headline =x_heading_val.reshape(9993,MAX_SENTS *MAX_SENT_LENGTH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEwKdC2vAiBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#concatenating headline and body before passing to the model\n",
        "x_train_m2= np.concatenate((x_train_headline,x_train_model1),axis=1)\n",
        "x_val_m2 = np.concatenate((x_val_headline,x_val_model1),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HbZPT4eB9fs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15e29870-a7f6-4088-e73e-5702445c3e53"
      },
      "source": [
        "x_val_m2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9993, 800)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFTIgKljw2XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Input layer containg both body and headline\n",
        "main_input_m2 = Input(shape=(800,), dtype='int32')\n",
        "\n",
        "# This embedding layer will encode the input sequence\n",
        "# into a sequence of dense 100-dimensional vectors.\n",
        "main_embed_m2 = Embedding(output_dim=100, input_dim=vocab_size, input_length=(800,))(main_input_m2)\n",
        "\n",
        "l_lstm_m2 = Bidirectional(LSTM(100,return_sequences=True))(main_embed_m2)\n",
        "l_dense_m22 = TimeDistributed(Dense(100))(l_lstm_m2)\n",
        "l_flatten_m2 = Flatten()(l_dense_m22)\n",
        "l_dense_m2 = Dense(4,activation='softmax')(l_flatten_m2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RP-KTSXzZOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model3 = Model(inputs=main_input_m2, outputs=l_dense_m2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuAKys5Z3m1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5993982a-2afb-4bd0-9679-590364347c2b"
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "embedding_14 (Embedding)     (None, 800, 100)          2787300   \n",
            "_________________________________________________________________\n",
            "bidirectional_10 (Bidirectio (None, 800, 200)          160800    \n",
            "_________________________________________________________________\n",
            "time_distributed_8 (TimeDist (None, 800, 100)          20100     \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 80000)             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4)                 320004    \n",
            "=================================================================\n",
            "Total params: 3,288,204\n",
            "Trainable params: 3,288,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c440aKld34ms",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compiling the model\n",
        "model3.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_9Kk7_Z9mSs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "517a5193-8228-4d77-c535-23b764f91b7b"
      },
      "source": [
        "type(x_train_headline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtWo4EkC4C00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "1e16341c-6cc5-4bf6-861e-9899cb89a8d0"
      },
      "source": [
        "model3.fit(x_train_m2, y=y_train,batch_size=32, epochs=2, verbose=1, validation_data=(x_val_m2,y_val), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 39978 samples, validate on 9993 samples\n",
            "Epoch 1/2\n",
            "39978/39978 [==============================] - 3985s 100ms/step - loss: 0.2676 - acc: 0.8946 - val_loss: 0.2367 - val_acc: 0.9072\n",
            "Epoch 2/2\n",
            "39978/39978 [==============================] - 3924s 98ms/step - loss: 0.2136 - acc: 0.9154 - val_loss: 0.2385 - val_acc: 0.9047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7bddffde48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEkz5GP7tT1N",
        "colab_type": "text"
      },
      "source": [
        "Model.fit results:\n",
        "\n",
        "Train on 39978 samples, validate on 9993 samples\n",
        "\n",
        "Epoch 1/2\n",
        "\n",
        "39978/39978 [==============================] - 3985s 100ms/step - loss: 0.2676 - acc: 0.8946 - val_loss: 0.2367 - val_acc: 0.9072\n",
        "\n",
        "Epoch 2/2\n",
        "\n",
        "39978/39978 [==============================] - 3924s 98ms/step - loss: 0.2136 - acc: 0.9154 - val_loss: 0.2385 - val_acc: 0.9047\n",
        "<keras.callbacks.History at 0x7f7bddffde48>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psDTjmETi83y",
        "colab_type": "text"
      },
      "source": [
        "**MODEL 2:**\n",
        "\n",
        "This model uses both the  article body and its respective heading for prediction.The input is concatenated array of both heading and body which is then passed through an\n",
        "LSTM block. It has a batch size of 32 and produces 90.47% validation accuracy after 2 epochs.\n",
        "\n",
        "It can be noted that the model performs better when both the article heading and body is used to train the model, than when only the article body is used.\n",
        "\n",
        "**MODEL SUMMARY:**\n",
        "\n",
        "Model: \"model_8\"\n",
        "\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "\n",
        "input_6 (InputLayer)         (None, 800)               0         \n",
        "_________________________________________________________________\n",
        "embedding_14 (Embedding)     (None, 800, 100)          2787300   \n",
        "_________________________________________________________________\n",
        "bidirectional_10 (Bidirectio (None, 800, 200)          160800    \n",
        "_________________________________________________________________\n",
        "time_distributed_8 (TimeDist (None, 800, 100)          20100     \n",
        "_________________________________________________________________\n",
        "flatten_8 (Flatten)          (None, 80000)             0         \n",
        "_________________________________________________________________\n",
        "dense_16 (Dense)             (None, 4)                 320004    \n",
        "\n",
        "\n",
        "Total params: 3,288,204\n",
        "Trainable params: 3,288,204\n",
        "Non-trainable params: 0\n",
        "____________________________\n",
        "\n",
        "**MODEL RESULTS:**\n",
        "\n",
        "Train on 39978 samples, validate on 9993 samples\n",
        "Epoch 1/2\n",
        "39978/39978 [==============================] - 3985s 100ms/step - loss: 0.2676 - acc: 0.8946 - val_loss: 0.2367 - val_acc: 0.9072\n",
        "\n",
        "Epoch 2/2\n",
        "39978/39978 [==============================] - 3924s 98ms/step - loss: 0.2136 - acc: 0.9154 - val_loss: 0.2385 - val_acc: 0.9047\n",
        "<keras.callbacks.History at 0x7f7bddffde48>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAi8Jq_KuV9o",
        "colab_type": "text"
      },
      "source": [
        "**MODEL 3**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vo9YQSXb9yqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input layer containg both body and headline\n",
        "main_input_m3 = Input(shape=(800,), dtype='int32')\n",
        "\n",
        "# This embedding layer will encode the input sequence\n",
        "# into a sequence of dense 100-dimensional vectors.\n",
        "main_embed_m3= Embedding(output_dim=100, input_dim=vocab_size, input_length=(800,))(main_input_m3)\n",
        "\n",
        "l_lstm_m3 = Bidirectional(LSTM(100,return_sequences=True))(main_embed_m3)\n",
        "l_dense_m33 = TimeDistributed(Dense(100))(l_lstm_m3)\n",
        "l_flatten_m3 = Flatten()(l_dense_m33)\n",
        "\n",
        "\n",
        "# Adding a dropout layer before the dense layer\n",
        "dropout = Dropout(0.2)(l_flatten_m3)\n",
        "#Output layer uses softmax activation function\n",
        "l_out_m3 = Dense(4,activation='softmax')(dropout)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5Kj1OV8w4B8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model4 = Model(inputs=main_input_m3, outputs=l_out_m3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbDne0vPxKym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compiling the model\n",
        "model4.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRG15YUnw5L_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "4fc9d946-95cd-4d10-f85d-cb8129d9d5fe"
      },
      "source": [
        "#fitting the model ,increasing the batch size to 50\n",
        "model4.fit(x_train_m2, y=y_train,batch_size=50, epochs=2, verbose=1, validation_data=(x_val_m2,y_val), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 39978 samples, validate on 9993 samples\n",
            "Epoch 1/2\n",
            "39978/39978 [==============================] - 2545s 64ms/step - loss: 0.2679 - acc: 0.8940 - val_loss: 0.2346 - val_acc: 0.9075\n",
            "Epoch 2/2\n",
            "39978/39978 [==============================] - 2544s 64ms/step - loss: 0.2104 - acc: 0.9168 - val_loss: 0.2374 - val_acc: 0.9075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7ff546e4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfpXXW-S_e33",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b1342673-4e89-4e6c-c7df-12e299570ea4"
      },
      "source": [
        "model4.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 800, 100)          2787300   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 800, 200)          160800    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 800, 100)          20100     \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 80000)             0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 80000)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 320004    \n",
            "=================================================================\n",
            "Total params: 3,288,204\n",
            "Trainable params: 3,288,204\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0k9qXNW8WwK",
        "colab_type": "text"
      },
      "source": [
        "**MODEL 3:**\n",
        "\n",
        "This model also takes the combined array of both headline and body as input. It has an additional dropout layer just before the output layer, which uses softmax activation function. Batch size used for training is 50. The model achieves a validation accuracy of 90.75% after 2 epochs of training.\n",
        "\n",
        "Adding the dropout layer only gives very minute improvement in model accuracy, but reduces the the time required for training.\n",
        "\n",
        "**Model Summary:**\n",
        "Model: \"model_2\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "input_2 (InputLayer)         (None, 800)               0         \n",
        "_________________________________________________________________\n",
        "embedding_2 (Embedding)      (None, 800, 100)          2787300   \n",
        "_________________________________________________________________\n",
        "bidirectional_2 (Bidirection (None, 800, 200)          160800    \n",
        "_________________________________________________________________\n",
        "time_distributed_2 (TimeDist (None, 800, 100)          20100     \n",
        "_________________________________________________________________\n",
        "flatten_2 (Flatten)          (None, 80000)             0         \n",
        "_________________________________________________________________\n",
        "dropout_2 (Dropout)          (None, 80000)             0         \n",
        "_________________________________________________________________\n",
        "dense_4 (Dense)              (None, 4)                 320004    \n",
        "\n",
        "Total params: 3,288,204\n",
        "Trainable params: 3,288,204\n",
        "Non-trainable params: 0\n",
        "\n",
        "**Model Results:**\n",
        "\n",
        "Train on 39978 samples, validate on 9993 samples\n",
        "\n",
        "Epoch 1/2\n",
        "\n",
        "39978/39978 [==============================] - 2545s 64ms/step - loss: 0.2679 - acc: 0.8940 - val_loss: 0.2346 - val_acc: 0.9075\n",
        "\n",
        "Epoch 2/2\n",
        "\n",
        "39978/39978 [==============================] - 2544s 64ms/step - loss: 0.2104 - acc: 0.9168 - val_loss: 0.2374 - val_acc: 0.9075\n",
        "\n",
        "<keras.callbacks.History at 0x7f7ff546e4e0>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So_kVtOeLJKf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7d0e4ae8-d3f5-47a9-9bb1-9aa22e3827da"
      },
      "source": [
        "print(\"validation data shape:\",x_val_m2.shape)\n",
        "x_val_first= x_val_m2[0:1]\n",
        "print(\"first data shape:\",x_val_first.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation data shape: (9993, 800)\n",
            "first data shape: (1, 800)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb38xC-dK0Io",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predicting the label for a news article\n",
        "ynew = model4.predict(x_val_first)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjHAmlNfNHSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d946b851-1317-46da-8e41-ce3dd680b789"
      },
      "source": [
        "#predicting output\n",
        "labels=pd.get_dummies(dataset['Stance'])\n",
        "print('Predicted label for data at index 0:',labels.columns[ynew.argmax()])\n",
        "print('Actual label for data at index 0:',labels.columns[y_val[0].argmax()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted label for data at index 0: unrelated\n",
            "Actual label for data at index 0: unrelated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5XxjT5IQuq7",
        "colab_type": "text"
      },
      "source": [
        "**MODEL 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4yw3-EpHSVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Input layer containg both body and headline\n",
        "main_input_m4 = Input(shape=(800,), dtype='int32')\n",
        "\n",
        "# This embedding layer will encode the input sequence\n",
        "# into a sequence of dense 100-dimensional vectors.\n",
        "main_embed_m4= Embedding(output_dim=100, input_dim=vocab_size, input_length=(800,))(main_input_m4)\n",
        "#Adding a dropout of 0.2\n",
        "dropout1 = Dropout(0.2)(main_embed_m4)\n",
        "l_lstm_m4 = Bidirectional(LSTM(100,return_sequences=True))(dropout1)\n",
        "#Adding a Normalization layer\n",
        "l_norm_m4 = BatchNormalization()(l_lstm_m4)\n",
        "l_dense_m44 = TimeDistributed(Dense(100))(l_norm_m4 )\n",
        "l_flatten_m4 = Flatten()(l_dense_m44)\n",
        "\n",
        "\n",
        "# Adding a dropout layer before the dense layer\n",
        "dropout4 = Dropout(0.2)(l_flatten_m4)\n",
        "#Output layer uses softmax activation function\n",
        "l_out_m4 = Dense(4,activation='softmax')(dropout4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50hIuZksIsvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model5 = Model(inputs=main_input_m4, outputs=l_out_m4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2s30TO8QRZt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compiling the model\n",
        "model5.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQAtQdxmSWjc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "outputId": "b54bd15f-9139-4141-8cde-8de91233c805"
      },
      "source": [
        "model5.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "embedding_6 (Embedding)      (None, 800, 100)          2787300   \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 800, 100)          0         \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 800, 200)          160800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 800, 200)          800       \n",
            "_________________________________________________________________\n",
            "time_distributed_5 (TimeDist (None, 800, 100)          20100     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 80000)             0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 80000)             0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 4)                 320004    \n",
            "=================================================================\n",
            "Total params: 3,289,004\n",
            "Trainable params: 3,288,604\n",
            "Non-trainable params: 400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOMNOm-JSD6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "011809f5-ee29-45f4-9ea8-6756a1d7267d"
      },
      "source": [
        "#fitting the model ,increasing the batch size to 50\n",
        "model5.fit(x_train_m2, y=y_train,batch_size=50, epochs=2, verbose=1, validation_data=(x_val_m2,y_val), shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 39978 samples, validate on 9993 samples\n",
            "Epoch 1/2\n",
            "39978/39978 [==============================] - 2536s 63ms/step - loss: 0.3281 - acc: 0.8811 - val_loss: 0.2569 - val_acc: 0.9030\n",
            "Epoch 2/2\n",
            "39978/39978 [==============================] - 2544s 64ms/step - loss: 0.2297 - acc: 0.9098 - val_loss: 0.2500 - val_acc: 0.9038\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7fa75830f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbXdbsKFSaAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "664260cd-3e3f-410a-eb8b-c744c8d9eec5"
      },
      "source": [
        "#predicting the label for a news article\n",
        "prediction = model5.predict(x_val_first)\n",
        "\n",
        "#predicting output\n",
        "labels=pd.get_dummies(dataset['Stance'])\n",
        "print('Predicted label for data at index 0:',labels.columns[prediction.argmax()])\n",
        "print('Actual label for data at index 0:',labels.columns[y_val[0].argmax()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted label for data at index 0: unrelated\n",
            "Actual label for data at index 0: unrelated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJlxSlT4VxlF",
        "colab_type": "text"
      },
      "source": [
        "**MODEL 4**\n",
        "\n",
        "Model 4 also uses the conatenated array of headlines and article bodies. It has added layers of BatchNormalization and Dropout in addition to the LSTM, compared to model 3. Model 4 has a validation accuracy of 90.38%.\n",
        "\n",
        "Adding Batch Normalization decreases the accuracy of the model by a minute value.\n",
        "\n",
        "**Model Summary:**\n",
        "\n",
        "Model: \"model_4\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                    Output Shape                Param    \n",
        "_________________________________________________________________\n",
        "input_6 (InputLayer)     (None, 800)               0         \n",
        "_________________________________________________________________\n",
        "embedding_6 (Embedding)      (None, 800, 100)          2787300   \n",
        "_________________________________________________________________\n",
        "dropout_7 (Dropout)          (None, 800, 100)          0         \n",
        "_________________________________________________________________\n",
        "bidirectional_6 (Bidirection (None, 800, 200)          160800    \n",
        "_________________________________________________________________\n",
        "batch_normalization_3 (Batch (None, 800, 200)          800       \n",
        "_________________________________________________________________\n",
        "time_distributed_5 (TimeDist (None, 800, 100)          20100     \n",
        "_________________________________________________________________\n",
        "flatten_4 (Flatten)          (None, 80000)             0         \n",
        "_________________________________________________________________\n",
        "dropout_8 (Dropout)          (None, 80000)             0         \n",
        "_________________________________________________________________\n",
        "dense_9 (Dense)              (None, 4)                 320004    \n",
        "\n",
        " \n",
        "Total params: 3,289,004\n",
        "Trainable params: 3,288,604\n",
        "Non-trainable params: 400\n",
        "\n",
        "**Model Results:**\n",
        "\n",
        "Train on 39978 samples, validate on 9993 samples\n",
        "\n",
        "Epoch 1/2\n",
        "\n",
        "39978/39978 [==============================] - 2536s 63ms/step - loss: 0.3281 - acc: 0.8811 - val_loss: 0.2569 - val_acc: 0.9030\n",
        "\n",
        "Epoch 2/2\n",
        "\n",
        "39978/39978 [==============================] - 2544s 64ms/step - loss: 0.2297 - acc: 0.9098 - val_loss: 0.2500 - val_acc: 0.9038\n",
        "\n",
        "<keras.callbacks.History at 0x7f7fa75830f0>\n"
      ]
    }
  ]
}